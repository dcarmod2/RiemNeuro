{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dodtin/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda3/envs/dodtin/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip, pickle\n",
    "import numpy as np\n",
    "from tsCluster import tsCluster as tsc\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from notebooks.helper import get_onehot_codes\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyriemann\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy\n",
    "\n",
    "#\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#pip install pyriemann\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.utils.distance import distance_riemann\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_4_old(str_inp):\n",
    "    if str_inp == 'ctr':\n",
    "        return 0\n",
    "    if str_inp == 'hl':\n",
    "        return 1\n",
    "    if str_inp == 'tin_hl':\n",
    "        return 2\n",
    "    if str_inp == 'tin':\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_8(str_inp,loc):\n",
    "    if loc == 'chm':\n",
    "        to_add = 0\n",
    "    elif loc == 'san':\n",
    "        to_add = 1\n",
    "    else:\n",
    "        raise ValueError(\"Invalid location\")\n",
    "    \n",
    "    if str_inp == 'ctr':\n",
    "        return 0 + to_add\n",
    "    if str_inp == 'hl':\n",
    "        return 1 + to_add\n",
    "    if str_inp == 'tin_hl':\n",
    "        return 2 + to_add\n",
    "    if str_inp == 'tin':\n",
    "        return 3 + to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_4(str_inp):\n",
    "    return str_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_4_tin_thresh(str_inp,name,metadata):\n",
    "    if str_inp == 'ctr':\n",
    "        return 0\n",
    "    if str_inp == 'hl':\n",
    "        return 1\n",
    "    if str_inp == 'tin_hl':\n",
    "        if metadata.loc[int(re.sub(r'sub_','',name)),'TFI_A'] <= 25:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    if str_inp == 'tin':\n",
    "        if metadata.loc[int(re.sub(r'sub_','',name)),'TFI_A'] <= 25:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_4_tin_thresh2(str_inp,name,metadata):\n",
    "    if str_inp == 'ctr':\n",
    "        return 0\n",
    "    if str_inp == 'hl':\n",
    "        return 1\n",
    "    if str_inp == 'tin_hl' or str_inp == 'tin':\n",
    "        if metadata.loc[int(re.sub(r'sub_','',name)),'TFI_A'] <= 25:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create result directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('RiemannResults')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'bin_data/'\n",
    "fnames = ['wgsr_data33_tv_linear', 'nogsr_data33_tv_linear', 'wgsr_band33_tv_linear', 'nogsr_band33_tv_linear', 'wgsr_20band33_tv_linear', 'nogsr_20band33_tv_linear']\n",
    "\n",
    "for fname in fnames:\n",
    "    try:\n",
    "        os.mkdir('RiemannResults/'+fname)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of features is larger than number of samples, so linear kernel should be used in svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_runs_pre = pd.read_csv('Valid_Runs.csv',index_col='ID',header=0,names=['ID','A','B','C','D','E','F','G'],usecols=['ID','A','B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_runs_dict = {re.sub(r'\\d+_','',idx.rstrip('*')): row.values for idx,row in valid_runs_pre.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['113', '115', '117', '120', '122', '123', '126', '166', '167', '168', '169', '199', '200', '220', '258', '259', '261', '275', '125', '189', '191', '201', '226', '230', '243', '253', '265', '270', '271', '273', '284', '285', '103', '130', '135', '136', '150', '157', '182', '216', '218', '233', '274', '276', '286', '287', '291', '292', '102', '105', '106', '108', '129', '131', '133', '140', '146', '152', '153', '154', '158', '160', '162', '163', '174', '176', '178', '184', '187', '188', '197', '205', '207', '208', '210', '213', '217', '219', '222', '223', '227', '246', '247', '255', '256', '257', '277', '279', '280', '282', '293'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_runs_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'metadata_py/UIUC-Table 1.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-86bf3c66e666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetadata_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metadata_py/UIUC-Table 1.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmetadata_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metadata_py/WHASC-Table 1.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dodtin/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dodtin/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dodtin/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dodtin/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dodtin/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'metadata_py/UIUC-Table 1.csv' does not exist"
     ]
    }
   ],
   "source": [
    "metadata_1 = pd.read_csv('metadata_py/UIUC-Table 1.csv',index_col = 'ID')\n",
    "metadata_2 = pd.read_csv('metadata_py/WHASC-Table 1.csv',index_col = 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.concat([metadata_1,metadata_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(data_dir+fnames[0] + '.bin', 'rb') as stream:\n",
    "        data = pickle.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sub_257'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.samples[0]['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.samples[0]['Run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x['Location'] for x in data.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_selector(sample):\n",
    "    cond1 = int(sample['Run'].lstrip('run')) in valid_runs_dict.get(re.sub(r'sub_','',sample['Name']),[])\n",
    "    cond2 = int(sample['Run'].lstrip('run')) <= 2\n",
    "    return cond1 and cond2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class PCA_coord_change(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.pca_instance = PCA()\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        \n",
    "        self.pca_instance.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        shape = X.shape\n",
    "        coords = self.pca_instance.components_\n",
    "        coord_shape = coords.shape\n",
    "        if coord_shape[1] < shape[0]:\n",
    "            coords_padded = np.hstack([coords, np.zeros((coord_shape[0],-coord_shape[1]+shape[0]))])\n",
    "            X_padded = X\n",
    "        else:\n",
    "            coords_padded = coords\n",
    "            X_padded = np.vstack([X,np.zeros((coord_shape[1]-shape[0],shape[1]))])\n",
    "        return np.dot(coords_padded,X_padded)\n",
    "       \n",
    "    \n",
    "    def fit_transform(self,X,y=None,sample_weight=None):\n",
    "        \n",
    "        self.pca_instance.fit(X,y)\n",
    "        shape = X.shape\n",
    "        coords = self.pca_instance.components_\n",
    "        coord_shape = coords.shape\n",
    "        if coord_shape[1] < shape[0]:\n",
    "            coords_padded = np.hstack([coords, np.zeros((coord_shape[0],-coord_shape[1]+shape[0]))])\n",
    "            X_padded = X\n",
    "        else:\n",
    "            coords_padded = coords\n",
    "            X_padded = np.vstack([X,np.zeros((coord_shape[1]-shape[0],shape[1]))])\n",
    "        return np.dot(coords_padded,X_padded)\n",
    "        \n",
    "    \n",
    "class to_upper_tri(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,k):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        shape = X[0].shape\n",
    "        inds = np.triu_indices(shape[0],self.k)\n",
    "        return np.array([x[inds].flatten() for x in X])\n",
    "    \n",
    "    def fit_transform(self,X,y=None,sample_weight=None):\n",
    "        shape = X[0].shape\n",
    "        inds = np.triu_indices(shape[0],self.k)\n",
    "        return np.array([x[inds].flatten() for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accDict = {}\n",
    "simDict = {}\n",
    "matDict = {}\n",
    "corrDict = {}\n",
    "spearDict = {}\n",
    "for fname in fnames:\n",
    "    with gzip.open(data_dir+fname + '.bin', 'rb') as stream:\n",
    "        data = pickle.load(stream)\n",
    "        \n",
    "    simArr = []\n",
    "        \n",
    "    # get time series data to make covariance matrices\n",
    "    X = np.array([sample['TimeSeries'] for sample in data.samples if data_selector(sample)])        \n",
    "    y = np.array([get_label_4_old(sample['Group']) for sample in data.samples if data_selector(sample)])\n",
    "    \n",
    "    # gsr seems to produce a rank deficient covariance matrix, so oas regularization is necessary\n",
    "    if fname.startswith('no'):\n",
    "        covest = Covariances()\n",
    "    else:\n",
    "        covest = Covariances('oas')\n",
    "    ts = TangentSpace()\n",
    "    svc = SVC(kernel='linear')\n",
    "    clf_riem = make_pipeline(covest,ts,svc)\n",
    "    \n",
    "    if fname.startswith('no'):\n",
    "        covest2 = Covariances()\n",
    "    else:\n",
    "        covest2 = Covariances('oas')\n",
    "    svc2 = SVC(kernel='linear')\n",
    "    get_tri_inds = to_upper_tri()\n",
    "    clf_cov = make_pipeline(covest2,get_tri_inds,svc2)\n",
    "        \n",
    "    X_base = np.array([sample['CRM'][np.triu_indices(sample['CRM'].shape[0],1)].flatten() for sample in data.samples if data_selector(sample)])\n",
    "    y_base = np.array([get_label_4_old(sample['Group']) for sample in data.samples if data_selector(sample)])\n",
    "    svc_base = SVC(kernel='linear')\n",
    "    clf_base = svc_base\n",
    "    \n",
    "    svc_pca = SVC(kernel='linear')\n",
    "    decomp_pca = PCA_coord_change()\n",
    "    clf_pca = make_pipeline(decomp_pca,svc_pca)\n",
    "    \n",
    "    X_lag = np.array([sample['LCM'][np.triu_indices(sample['LCM'].shape[0],1)].flatten() for sample in data.samples if data_selector(sample)])\n",
    "    y_lag = np.array([get_label_4_old(sample['Group']) for sample in data.samples if data_selector(sample)])\n",
    "    svc_lag = SVC(kernel='linear')\n",
    "    clf_lag = svc_lag\n",
    "    \n",
    "    clf_composite = SVC(kernel='linear')\n",
    "    \n",
    "    #Check clustering\n",
    "    #to_TS = make_pipeline(covest,ts)\n",
    "    #X_in_TS = to_TS.transform(X)\n",
    "    #kmeans = KMeans(n_clusters=4,random_state=0).fit(X_in_TS)\n",
    "            \n",
    "    # Monte Carlo, in theory should run this len(y)^2 times, but I need to save my poor computer's memory.\n",
    "    accRiemList = []\n",
    "    accCovList = []\n",
    "    accPcaList = []\n",
    "    accBaseList = []\n",
    "    accLagList = []\n",
    "    accCompList = []\n",
    "    coeffArr = []\n",
    "    matRiemList = []\n",
    "    matBaseList = []\n",
    "    matLagList = []\n",
    "    corrArrBefore = []\n",
    "    corrArrAfter = []\n",
    "    spearArrBefore = []\n",
    "    spearArrAfter = []\n",
    "\n",
    "    rs = StratifiedShuffleSplit(n_splits=100, test_size=.3)\n",
    "    for i,(train_inds,test_inds) in enumerate(rs.split(X,y)):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = X[train_inds],X[test_inds],y[train_inds],y[test_inds]\n",
    "        X_train_cov, X_test_cov, y_train_cov, y_test_cov = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()\n",
    "        X_train_base, X_test_base, y_train_base, y_test_base = X_base[train_inds],X_base[test_inds],y_base[train_inds],y_base[test_inds]\n",
    "        X_train_lag, X_test_lag, y_train_lag, y_test_lag = X_lag[train_inds],X_lag[test_inds],y_lag[train_inds],y_lag[test_inds]\n",
    "        \n",
    "        clf_riem.fit(X_train,y_train)\n",
    "        clf_cov.fit(X_train_cov,y_train_cov)\n",
    "        clf_pca.fit(X_train_base.copy(),y_train_base.copy())\n",
    "        clf_base.fit(X_train_base,y_train_base)\n",
    "        clf_lag.fit(X_train_lag,y_train_lag)\n",
    "        clf_composite.fit(X_train_lag,y_train_lag)\n",
    "        \n",
    "        #get riemann svm coefficients\n",
    "        coeffArr.append(clf_riem[2].coef_)\n",
    "            \n",
    "        #compare correlation\n",
    "        corr_coeffs_before = np.corrcoef(np.vstack([x[np.triu_indices(33)].flatten() for x in X_train]),rowvar=False)\n",
    "        corrArrBefore.append(np.linalg.norm(corr_coeffs_before))\n",
    "        #spearman correlation\n",
    "        spearman_coeffs_before,_ = scipy.stats.spearmanr(np.vstack([x[np.triu_indices(33)].flatten() for x in X_train]),axis=0)\n",
    "        spearArrBefore.append(np.linalg.norm(spearman_coeffs_before))\n",
    "        \n",
    "        ref = ts.reference_\n",
    "        covs = covest.transform(X_train)\n",
    "        mapped = ts.transform(covs)\n",
    "        corr_coeffs_after = np.corrcoef(mapped,rowvar=False)\n",
    "        spearman_coeffs_after = scipy.stats.spearmanr(mapped,axis=0)\n",
    "        corrArrAfter.append(np.linalg.norm(corr_coeffs_after))\n",
    "        spearArrAfter.append(np.linalg.norm(spearman_coeffs_after))\n",
    "        \n",
    "        #for flattened in mapped:\n",
    "            \n",
    "            #mat = np.zeros((33,33))\n",
    "            #mat[np.triu_indices(33)] = flattened\n",
    "            #mat = (mat + mat.T)/2\n",
    "            #mapped_mats.append(mat)\n",
    "            \n",
    "        #print(np.mean(np.array([distance_riemann(x,ref) for x in covs])-np.array([np.linalg.norm(x,ord='fro') for x in mapped_mats])/np.array([distance_riemann(x,ref) for x in covs])))\n",
    "        \n",
    "        y_pred = clf_riem.predict(X_test)\n",
    "        y_pred_cov = clf_cov.predict(X_test_cov)\n",
    "        y_pred_pca = clf_pca.predict(X_test_base.copy())\n",
    "        y_pred_base = clf_base.predict(X_test_base)\n",
    "        y_pred_lag = clf_lag.predict(X_test_lag)\n",
    "        \n",
    "        # save accuracy\n",
    "        accRiemList.append(accuracy_score(y_pred,y_test))\n",
    "        accCovList.append(accuracy_score(y_pred_cov,y_test_cov))\n",
    "        accPcaList.append(accuracy_score(y_pred_pca,y_test_base.copy()))\n",
    "        accBaseList.append(accuracy_score(y_pred_base,y_test_base))\n",
    "        accLagList.append(accuracy_score(y_pred_lag,y_test_lag))\n",
    "        accCompList.append(accuracy_score(clf_composite.predict(X_test_base),y_test_base))\n",
    "        \n",
    "        # confusion matrix\n",
    "        mat = confusion_matrix(y_test,y_pred,normalize='true',labels=[0,1,2,3])\n",
    "        #plot_confusion_matrix(clf_riem, X_test, y_test,normalize='true',labels=['ctr','hl','tin_hl','tin'],display_labels=['ctr','ctr_hl','hl_tin','tin'])\n",
    "        #plt.savefig('RiemannResults/' + fname + '/ConfMatRiemRun' + str(i) + '.png',bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        mat_base = confusion_matrix(y_test_base,y_pred_base,normalize='true',labels=[0,1,2,3])\n",
    "        #plot_confusion_matrix(clf_base, X_test_base, y_test_base,normalize='true',labels=['ctr','hl','tin_hl','tin'],display_labels=['ctr','ctr_hl','hl_tin','tin'])\n",
    "        #plt.savefig('RiemannResults/' + fname + '/ConfMatBaseRun' + str(i) + '.png',bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        mat_lag = confusion_matrix(y_test_lag,y_pred_lag,normalize='true',labels=[0,1,2,3])\n",
    "        #plot_confusion_matrix(clf_lag, X_test_lag, y_test_lag,normalize='true',labels=['ctr','hl','tin_hl','tin'],display_labels=['ctr','ctr_hl','hl_tin','tin'])\n",
    "        #plt.savefig('RiemannResults/' + fname + '/ConfMatLagRun' + str(i) + '.png',bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        \n",
    "        matRiemList.append(mat)\n",
    "        matBaseList.append(mat_base)\n",
    "        matLagList.append(mat_lag)\n",
    "    \n",
    "    for z in range(0,len(coeffArr[0])):\n",
    "        class_z_coeffs = [x[z] for x in coeffArr]\n",
    "        cos_sim = cosine_similarity(class_z_coeffs)\n",
    "        upperTri = cos_sim[np.triu_indices(cos_sim.shape[0],1)]\n",
    "        cos_avg = np.mean(upperTri.flatten())\n",
    "        simArr.append(cos_avg)\n",
    "        \n",
    "    avgMatRiem = sum(matRiemList)/len(matRiemList)\n",
    "    simDict.update({fname: simArr}) \n",
    "    matDict.update({fname: avgMatRiem})\n",
    "    riemAcc = np.mean(accRiemList)\n",
    "    covAcc = np.mean(accCovList)\n",
    "    pcaAcc = np.mean(accPcaList)\n",
    "    baseAcc = np.mean(accBaseList)\n",
    "    lagAcc = np.mean(accLagList)\n",
    "    compAcc = np.mean(accCompList)\n",
    "    accDict.update({fname:{'riem':riemAcc, 'cov':covAcc, 'pca': pcaAcc, 'base': baseAcc,'lag': lagAcc,'comp':compAcc}})\n",
    "    corrDict.update({fname:{'before':np.mean(corrArrBefore),'after':np.mean(corrArrAfter)}}) \n",
    "    spearDict.update({fname:{'before':np.mean(spearArrBefore),'after':np.mean(spearArrAfter)}}) \n",
    "    print(\"Mean Accuracy w/ Riemann on file \" + fname + \": \" + str(riemAcc))\n",
    "    print(\"Mean Accuracy w/ Cov on file \" + fname + \": \" + str(covAcc))\n",
    "    print(\"Mean Accuracy w/ PCA on file \" + fname + \": \" + str(pcaAcc))\n",
    "    print(\"Mean Accuracy w/ Base on file \" + fname + \": \" + str(baseAcc))\n",
    "    print(\"Mean Accuracy w/ Lag on file \" + fname + \": \" + str(lagAcc))\n",
    "    print(\"Mean Accuracy w/ Composite on file \" + fname + \": \" + str(compAcc))\n",
    "    print(\"----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for fname,mat in matDict.items():   \n",
    "    plt.figure()\n",
    "    df_cm = pd.DataFrame(mat, index = ['ctr','ctr_hl','hl_tin','tin'],\n",
    "                      columns = ['ctr','ctr_hl','hl_tin','tin'])\n",
    "\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.title(fname + \" average confusion matrix\")\n",
    "    plt.ylabel('actual label')\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.savefig('RiemannResults/'+fname+'/AvgConfMat.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide by sum along rows to normalize by total predictions in a given class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname,mat in matDict.items():   \n",
    "    plt.figure()\n",
    "    df_cm = pd.DataFrame(mat/np.sum(mat,axis=0), index = ['ctr','ctr_hl','hl_tin','tin'],\n",
    "                      columns = ['ctr','ctr_hl','hl_tin','tin'])\n",
    "\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.title(fname + \" Pr(actual label | predicted label)\")\n",
    "    plt.ylabel('actual label')\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.savefig('RiemannResults/'+fname+'/ConditionalAvgConfMat.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get discriminative connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_pair(ind):\n",
    "    x,y = np.triu_indices(33)\n",
    "    return (x[ind],y[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig_pairs(data,coeffarr,nullcoeffs,p,maxmin=\"max\"):\n",
    "    rois = data.rois\n",
    "    if maxmin == \"max\":\n",
    "        percentile = 100-p\n",
    "    elif maxmin == \"min\":\n",
    "        percentile = p\n",
    "    thresholds = np.percentile(nullcoeffs,percentile,axis=0)\n",
    "    if maxmin == \"max\":\n",
    "        boolarr =  coeffarr > thresholds.reshape(6,1)\n",
    "    elif maxmin == \"min\":\n",
    "        boolarr = coeffarr < thresholds.reshape(6,1)\n",
    "    indarr = [[(rois[ind_to_pair(x)[0]],rois[ind_to_pair(x)[1]]) for i,x in enumerate(range(0,len(boolarr[0]))) if boolarr[j][i]] for j in range(0,len(boolarr))]\n",
    "    return indarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(data_dir+fnames[1] + '.bin', 'rb') as stream:\n",
    "        data = pickle.load(stream)\n",
    "\n",
    "# First, make null model\n",
    "# Use integer labeling so we can be sure that the one vs one classifiers are \n",
    "# in the correct orders\n",
    "X = np.array([sample['CRM'] for sample in data.samples if (sample['Run']=='run1' or sample['Run'] == 'run2')])        \n",
    "y = np.array([get_label_4_old(sample['Group']) for sample in data.samples if (sample['Run']=='run1' or sample['Run'] == 'run2')])\n",
    "# Randomly permute labels (only labels, not training input)\n",
    "\n",
    "NUM_BOOTSTRAP = 10\n",
    "svc = SVC(kernel='linear')\n",
    "tut= to_upper_tri(1)\n",
    "clf_riem = make_pipeline(tut,svc)\n",
    "maxcoeffs = []   \n",
    "mincoeffs = []\n",
    "nullcoeffs = []\n",
    "nullcos = []\n",
    "for i in range(0,100):\n",
    "    y_permuted = np.random.permutation(y)\n",
    "    coeffArr = []\n",
    "    rs = ShuffleSplit(n_splits=NUM_BOOTSTRAP, test_size=.3)\n",
    "    for train,test in rs.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train],X[test],y_permuted[train],y_permuted[test]\n",
    "        clf_riem.fit(X_train,y_train)\n",
    "        coeffArr.append(clf_riem[1].coef_/np.std(clf_riem[1].coef_,axis=-1).reshape(6,1))\n",
    "        \n",
    "    meancoeff = sum(coeffArr)/len(coeffArr)\n",
    "    classcos = []\n",
    "    for z in range(0,len(coeffArr[0])):\n",
    "        class_z_coeffs = [x[z] for x in coeffArr]\n",
    "        cos_sim = cosine_similarity(class_z_coeffs)\n",
    "        upperTri = cos_sim[np.triu_indices(cos_sim.shape[0],1)]\n",
    "        cos_max = np.max(upperTri.flatten())\n",
    "        classcos.append(cos_max)\n",
    "        \n",
    "    nullcos.append(classcos)\n",
    "    nullcoeffs.append(meancoeff)\n",
    "    maxcoeff = np.max(meancoeff,axis=-1)\n",
    "    mincoeff = np.min(meancoeff,axis=-1)\n",
    "    maxcoeffs.append(maxcoeff)\n",
    "    mincoeffs.append(mincoeff)\n",
    "    \n",
    "coeffArr = []\n",
    "rs = ShuffleSplit(n_splits=NUM_BOOTSTRAP, test_size=.3)\n",
    "for train,test in rs.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train],X[test],y[train],y[test]\n",
    "    clf_riem.fit(X_train,y_train)\n",
    "    coeffArr.append(clf_riem[1].coef_/np.std(clf_riem[1].coef_,axis=-1).reshape(6,1))\n",
    "meancoeff = sum(coeffArr)/len(coeffArr)\n",
    "classcos = []\n",
    "for z in range(0,len(coeffArr[0])):\n",
    "    class_z_coeffs = [x[z] for x in coeffArr]\n",
    "    cos_sim = cosine_similarity(class_z_coeffs)\n",
    "    upperTri = cos_sim[np.triu_indices(cos_sim.shape[0],1)]\n",
    "    cos_max = np.max(upperTri.flatten())\n",
    "    classcos.append(upperTri.flatten())\n",
    "cossims=[classcos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_pairs_max = get_sig_pairs(data,meancoeff,maxcoeffs,10,\"max\")\n",
    "sig_pairs_min = get_sig_pairs(data,meancoeff,mincoeffs,10,\"min\")\n",
    "cmpr_list = ['ctr_vs_ctr_hl','ctr_vs_tin_hl','ctr_vs_tin','ctr_hl_vs_tin_hl','ctr_hl_vs_tin','tin_hl_vs_tin']\n",
    "discrim_dict_max = {cmpr_list[i]: sig_pairs_max[i] for i in range(0,6)}\n",
    "discrim_dict_min = {cmpr_list[i]: sig_pairs_min[i] for i in range(0,6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctr_vs_ctr_hl': [],\n",
       " 'ctr_vs_tin_hl': [],\n",
       " 'ctr_vs_tin': [('lsuperiortemporaljunction', 'ramygdala')],\n",
       " 'ctr_hl_vs_tin_hl': [],\n",
       " 'ctr_hl_vs_tin': [],\n",
       " 'tin_hl_vs_tin': [('lmidfrontalgyrus', 'ranteriorinsula'),\n",
       "  ('medialprefrontalcortex', 'rfrontaleyefield')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrim_dict_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctr_vs_ctr_hl': [('lmidfrontalgyrus', 'lprimaryauditorycortex')],\n",
       " 'ctr_vs_tin_hl': [('lsuperioroccipitallobe', 'rmidfrontalgyrus')],\n",
       " 'ctr_vs_tin': [],\n",
       " 'ctr_hl_vs_tin_hl': [],\n",
       " 'ctr_hl_vs_tin': [('lamygdala', 'medialprefrontalcortex')],\n",
       " 'tin_hl_vs_tin': []}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrim_dict_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(data_dir+fnames[1] + '.bin', 'rb') as stream:\n",
    "        data = pickle.load(stream)\n",
    "\n",
    "# First, make null model\n",
    "# Use integer labeling so we can be sure that the one vs one classifiers are \n",
    "# in the correct orders\n",
    "X = np.array([sample['TimeSeries'] for sample in data.samples if (sample['Run']=='run1' or sample['Run'] == 'run2')])        \n",
    "y = np.array([get_label_4_old(sample['Group']) for sample in data.samples if (sample['Run']=='run1' or sample['Run'] == 'run2')])\n",
    "# Randomly permute labels (only labels, not training input)\n",
    "\n",
    "NUM_BOOTSTRAP = 10\n",
    "covest = Covariances()\n",
    "ts = TangentSpace()\n",
    "svc = SVC(kernel='linear')\n",
    "clf_riem = make_pipeline(covest,ts,svc)\n",
    "maxcoeffs = []   \n",
    "mincoeffs = []\n",
    "nullcoeffs = []\n",
    "nullcos = []\n",
    "for i in range(0,100):\n",
    "    y_permuted = np.random.permutation(y)\n",
    "    coeffArr = []\n",
    "    rs = ShuffleSplit(n_splits=NUM_BOOTSTRAP, test_size=.3)\n",
    "    for train,test in rs.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train],X[test],y_permuted[train],y_permuted[test]\n",
    "        clf_riem.fit(X_train,y_train)\n",
    "        coeffArr.append(clf_riem[2].coef_/np.std(clf_riem[2].coef_,axis=-1).reshape(6,1))\n",
    "        \n",
    "    meancoeff = sum(coeffArr)/len(coeffArr)\n",
    "    classcos = []\n",
    "    for z in range(0,len(coeffArr[0])):\n",
    "        class_z_coeffs = [x[z] for x in coeffArr]\n",
    "        cos_sim = cosine_similarity(class_z_coeffs)\n",
    "        upperTri = cos_sim[np.triu_indices(cos_sim.shape[0],1)]\n",
    "        cos_max = np.max(upperTri.flatten())\n",
    "        classcos.append(cos_max)\n",
    "        \n",
    "    nullcos.append(classcos)\n",
    "    nullcoeffs.append(meancoeff)\n",
    "    maxcoeff = np.max(meancoeff,axis=-1)\n",
    "    mincoeff = np.min(meancoeff,axis=-1)\n",
    "    maxcoeffs.append(maxcoeff)\n",
    "    mincoeffs.append(mincoeff)\n",
    "    \n",
    "coeffArr = []\n",
    "rs = ShuffleSplit(n_splits=NUM_BOOTSTRAP, test_size=.3)\n",
    "for train,test in rs.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train],X[test],y[train],y[test]\n",
    "    clf_riem.fit(X_train,y_train)\n",
    "    coeffArr.append(clf_riem[2].coef_/np.std(clf_riem[2].coef_,axis=-1).reshape(6,1))\n",
    "meancoeff = sum(coeffArr)/len(coeffArr)\n",
    "classcos = []\n",
    "for z in range(0,len(coeffArr[0])):\n",
    "    class_z_coeffs = [x[z] for x in coeffArr]\n",
    "    cos_sim = cosine_similarity(class_z_coeffs)\n",
    "    upperTri = cos_sim[np.triu_indices(cos_sim.shape[0],1)]\n",
    "    cos_max = np.max(upperTri.flatten())\n",
    "    classcos.append(upperTri.flatten())\n",
    "cossims=[classcos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cossims = [np.mean(x) for x in cossims[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8354182994766365,\n",
       " 0.7888007461782324,\n",
       " 0.8046822291302825,\n",
       " 0.7821434494688512,\n",
       " 0.8203338218959366,\n",
       " 0.7649699962585795]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_cossims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_dist = np.array(nullcos).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_dist_thresh = [np.percentile(x,97) for x in null_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7865834841596142,\n",
       " 0.7506129008005149,\n",
       " 0.7962413453967285,\n",
       " 0.7677454378167389,\n",
       " 0.8037777291430522,\n",
       " 0.7588356045210332]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_dist_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_cossims > null_dist_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_pairs_max = get_sig_pairs(data,meancoeff,maxcoeffs,5,\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_pairs_min = get_sig_pairs(data,meancoeff,mincoeffs,5,\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpr_list = ['ctr_vs_ctr_hl','ctr_vs_tin_hl','ctr_vs_tin','ctr_hl_vs_tin_hl','ctr_hl_vs_tin','tin_hl_vs_tin']\n",
    "discrim_dict_max = {cmpr_list[i]: sig_pairs_max[i] for i in range(0,6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctr_vs_ctr_hl': [('lventralintraparietalsulcus',\n",
       "   'lventralintraparietalsulcus'),\n",
       "  ('precuneus', 'rcuneus'),\n",
       "  ('rcuneus', 'rprimaryvisualcortex')],\n",
       " 'ctr_vs_tin_hl': [('posteriorcingulatecortex', 'posteriorcingulatecortex'),\n",
       "  ('rmidfrontalgyrus', 'rsuperiortemporalsulcus')],\n",
       " 'ctr_vs_tin': [('posteriorcingulatecortex', 'posteriorcingulatecortex')],\n",
       " 'ctr_hl_vs_tin_hl': [('rcuneus', 'rsuperioroccipitallobe'),\n",
       "  ('rfrontaleyefield', 'rfrontaleyefield')],\n",
       " 'ctr_hl_vs_tin': [('lamygdala', 'lamygdala'),\n",
       "  ('lprimaryvisualcortex', 'lprimaryvisualcortex'),\n",
       "  ('lprimaryvisualcortex', 'rcuneus')],\n",
       " 'tin_hl_vs_tin': [('lamygdala', 'rparahippocampus'),\n",
       "  ('rcuneus', 'rcuneus'),\n",
       "  ('rinferiorparietallobe', 'rsuperiortemporaljunction')]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrim_dict_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim_dict_min = {cmpr_list[i]: sig_pairs_min[i] for i in range(0,6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctr_vs_ctr_hl': [('lprimaryvisualcortex', 'lprimaryvisualcortex')],\n",
       " 'ctr_vs_tin_hl': [('lsuperiortemporaljunction', 'posteriorcingulatecortex'),\n",
       "  ('rmidfrontalgyrus', 'rmidfrontalgyrus')],\n",
       " 'ctr_vs_tin': [],\n",
       " 'ctr_hl_vs_tin_hl': [],\n",
       " 'ctr_hl_vs_tin': [],\n",
       " 'tin_hl_vs_tin': [('precuneus', 'precuneus')]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrim_dict_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barheights = [[x for x in valdict.values()][:-1] for filename,valdict in accDict.items()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bheights = np.array(barheights).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ['red','green','blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(x.rstrip('tv_linear') for x in accDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 40})\n",
    "fig,ax = plt.subplots(figsize=(12,12))\n",
    "ax.set_ylim(0,1.2)\n",
    "ax.grid(True)\n",
    "for i,filebar in enumerate(bheights):\n",
    "    ax.bar([i/3+2*j for j,_ in enumerate(bheights[0])],filebar,color = color[i], \n",
    "           width = 1/3,tick_label = keys)\n",
    "plt.xticks([1/3+2*j for j in range(0,6)], rotation=45)\n",
    "plt.legend(['Riemann','SVM corr','SVM lag corr'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classifier Accuracies')\n",
    "dx = -60/72.; dy = 0/72. \n",
    "offset = matplotlib.transforms.ScaledTranslation(dx, dy, fig.dpi_scale_trans)\n",
    "\n",
    "for label in ax.xaxis.get_majorticklabels():\n",
    "    label.set_transform(label.get_transform() + offset)\n",
    "    \n",
    "plt.savefig('RiemannResults/Graphs/ClassAcc.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use tin thresholds in class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDict = {}\n",
    "simDict = {}\n",
    "matDict = {}\n",
    "corrDict = {}\n",
    "for fname in fnames:\n",
    "    with gzip.open(data_dir+fname + '.bin', 'rb') as stream:\n",
    "        data = pickle.load(stream)\n",
    "        \n",
    "    simArr = []\n",
    "        \n",
    "    # get time series data to make covariance matrices\n",
    "    X = np.array([sample['TimeSeries'] for sample in data.samples if data_selector(sample)])        \n",
    "    y = np.array([get_label_4_tin_thresh(sample['Group'],sample['Name'],metadata) for sample in data.samples if data_selector(sample)])\n",
    "    \n",
    "    # gsr seems to produce a rank deficient covariance matrix, so oas regularization is necessary\n",
    "    if fname.startswith('no'):\n",
    "        covest = Covariances()\n",
    "    else:\n",
    "        covest = Covariances('oas')\n",
    "    ts = TangentSpace()\n",
    "    svc = SVC(kernel='linear')\n",
    "    clf_riem = make_pipeline(covest,ts,svc)\n",
    "        \n",
    "    X_base = np.array([sample['CRM'][np.triu_indices(sample['CRM'].shape[0])].flatten() for sample in data.samples if data_selector(sample)])\n",
    "    y_base = np.array([get_label_4_tin_thresh(sample['Group'],sample['Name'],metadata) for sample in data.samples if data_selector(sample)])\n",
    "    svc_base = SVC(kernel='linear')\n",
    "    clf_base = svc_base\n",
    "    \n",
    "    X_lag = np.array([sample['LCM'][np.triu_indices(sample['LCM'].shape[0])].flatten() for sample in data.samples if data_selector(sample)])\n",
    "    y_lag = np.array([get_label_4_tin_thresh(sample['Group'],sample['Name'],metadata) for sample in data.samples if data_selector(sample)])\n",
    "    svc_lag = SVC(kernel='linear')\n",
    "    clf_lag = svc_lag\n",
    "    \n",
    "    clf_composite = SVC(kernel='linear')\n",
    "    \n",
    "    #Check clustering\n",
    "    #to_TS = make_pipeline(covest,ts)\n",
    "    #X_in_TS = to_TS.transform(X)\n",
    "    #kmeans = KMeans(n_clusters=4,random_state=0).fit(X_in_TS)\n",
    "            \n",
    "    # Monte Carlo, in theory should run this len(y)^2 times, but I need to save my poor computer's memory.\n",
    "    accRiemList = []\n",
    "    accBaseList = []\n",
    "    accLagList = []\n",
    "    accCompList = []\n",
    "    coeffArr = []\n",
    "    matRiemList = []\n",
    "    matBaseList = []\n",
    "    matLagList = []\n",
    "    corrArrBefore = []\n",
    "    corrArrAfter = []\n",
    "\n",
    "    rs = StratifiedShuffleSplit(n_splits=100, test_size=.3)\n",
    "    for i,(train_inds,test_inds) in enumerate(rs.split(X,y)):\n",
    "        X_train, X_test, y_train, y_test = X[train_inds],X[test_inds],y[train_inds],y[test_inds]\n",
    "        X_train_base, X_test_base, y_train_base, y_test_base = X_base[train_inds],X_base[test_inds],y_base[train_inds],y_base[test_inds]\n",
    "        X_train_lag, X_test_lag, y_train_lag, y_test_lag = X_lag[train_inds],X_lag[test_inds],y_lag[train_inds],y_lag[test_inds]\n",
    "        \n",
    "        clf_riem.fit(X_train,y_train)\n",
    "        clf_base.fit(X_train_base,y_train_base)\n",
    "        clf_lag.fit(X_train_lag,y_train_lag)\n",
    "        clf_composite.fit(X_train_lag,y_train_lag)\n",
    "        \n",
    "        #get riemann svm coefficients\n",
    "        coeffArr.append(clf_riem[2].coef_)\n",
    "            \n",
    "        #compare correlation\n",
    "        corr_coeffs_before = np.corrcoef(np.vstack([x[np.triu_indices(33)].flatten() for x in X_train]),rowvar=False)\n",
    "        corrArrBefore.append(np.linalg.norm(corr_coeffs_before))\n",
    "        \n",
    "        ref = ts.reference_\n",
    "        covs = covest.transform(X_train)\n",
    "        mapped = ts.transform(covs)\n",
    "        corr_coeffs_after = np.corrcoef(mapped,rowvar=False)\n",
    "        corrArrAfter.append(np.linalg.norm(corr_coeffs_after))\n",
    "        \n",
    "        \n",
    "        #for flattened in mapped:\n",
    "            \n",
    "            #mat = np.zeros((33,33))\n",
    "            #mat[np.triu_indices(33)] = flattened\n",
    "            #mat = (mat + mat.T)/2\n",
    "            #mapped_mats.append(mat)\n",
    "            \n",
    "        #print(np.mean(np.array([distance_riemann(x,ref) for x in covs])-np.array([np.linalg.norm(x,ord='fro') for x in mapped_mats])/np.array([distance_riemann(x,ref) for x in covs])))\n",
    "        \n",
    "        y_pred = clf_riem.predict(X_test)\n",
    "        y_pred_base = clf_base.predict(X_test_base)\n",
    "        y_pred_lag = clf_lag.predict(X_test_lag)\n",
    "        \n",
    "        # save accuracy\n",
    "        accRiemList.append(accuracy_score(y_pred,y_test))\n",
    "        accBaseList.append(accuracy_score(y_pred_base,y_test_base))\n",
    "        accLagList.append(accuracy_score(y_pred_lag,y_test_lag))\n",
    "        accCompList.append(accuracy_score(clf_composite.predict(X_test_base),y_test_base))\n",
    "        \n",
    "        # confusion matrix\n",
    "        mat = confusion_matrix(y_test,y_pred,normalize='true',labels=[0,1,2,3])\n",
    "        #plot_confusion_matrix(clf_riem, X_test, y_test,normalize='true',labels=['ctr','hl','tin_hl','tin'],display_labels=['ctr','ctr_hl','hl_tin','tin'])\n",
    "        #plt.savefig('RiemannResults/' + fname + '/ConfMatRiemRun' + str(i) + '.png',bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        mat_base = confusion_matrix(y_test_base,y_pred_base,normalize='true',labels=[0,1,2,3])\n",
    "        #plot_confusion_matrix(clf_base, X_test_base, y_test_base,normalize='true',labels=['ctr','hl','tin_hl','tin'],display_labels=['ctr','ctr_hl','hl_tin','tin'])\n",
    "        #plt.savefig('RiemannResults/' + fname + '/ConfMatBaseRun' + str(i) + '.png',bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        mat_lag = confusion_matrix(y_test_lag,y_pred_lag,normalize='true',labels=[0,1,2,3])\n",
    "        #plot_confusion_matrix(clf_lag, X_test_lag, y_test_lag,normalize='true',labels=['ctr','hl','tin_hl','tin'],display_labels=['ctr','ctr_hl','hl_tin','tin'])\n",
    "        #plt.savefig('RiemannResults/' + fname + '/ConfMatLagRun' + str(i) + '.png',bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        \n",
    "        matRiemList.append(mat)\n",
    "        matBaseList.append(mat_base)\n",
    "        matLagList.append(mat_lag)\n",
    "    \n",
    "    for z in range(0,len(coeffArr[0])):\n",
    "        class_z_coeffs = [x[z] for x in coeffArr]\n",
    "        cos_sim = cosine_similarity(class_z_coeffs)\n",
    "        upperTri = cos_sim[np.triu_indices(cos_sim.shape[0],1)]\n",
    "        cos_avg = np.mean(upperTri.flatten())\n",
    "        simArr.append(cos_avg)\n",
    "        \n",
    "    avgMatRiem = sum(matRiemList)/len(matRiemList)\n",
    "    simDict.update({fname: simArr}) \n",
    "    matDict.update({fname: avgMatRiem})\n",
    "    riemAcc = np.mean(accRiemList)\n",
    "    baseAcc = np.mean(accBaseList)\n",
    "    lagAcc = np.mean(accLagList)\n",
    "    compAcc = np.mean(accCompList)\n",
    "    accDict.update({fname:{'riem':riemAcc, 'base': baseAcc,'lag': lagAcc,'comp':compAcc}})\n",
    "    corrDict.update({fname:{'before':np.mean(corrArrBefore),'after':np.mean(corrArrAfter)}})        \n",
    "    print(\"Mean Accuracy w/ Riemann on file \" + fname + \": \" + str(riemAcc))\n",
    "    print(\"Mean Accuracy w/ Base on file \" + fname + \": \" + str(baseAcc))\n",
    "    print(\"Mean Accuracy w/ Lag on file \" + fname + \": \" + str(lagAcc))\n",
    "    print(\"Mean Accuracy w/ Composite on file \" + fname + \": \" + str(compAcc))\n",
    "    print(\"----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDict = {}\n",
    "simDict = {}\n",
    "matDict = {}\n",
    "corrDict = {}\n",
    "for fname in fnames:\n",
    "    with gzip.open(data_dir+fname + '.bin', 'rb') as stream:\n",
    "        data = pickle.load(stream)\n",
    "        \n",
    "    simArr = []\n",
    "        \n",
    "    # get time series data to make covariance matrices\n",
    "    X = np.array([sample['TimeSeries'] for sample in data.samples if data_selector(sample)])        \n",
    "    y = np.array([get_label_4_tin_thresh2(sample['Group'],sample['Name'],metadata) for sample in data.samples if data_selector(sample)])\n",
    "    \n",
    "    # gsr seems to produce a rank deficient covariance matrix, so oas regularization is necessary\n",
    "    if fname.startswith('no'):\n",
    "        covest = Covariances()\n",
    "    else:\n",
    "        covest = Covariances('oas')\n",
    "    ts = TangentSpace()\n",
    "    svc = SVC(kernel='linear')\n",
    "    clf_riem = make_pipeline(covest,ts,svc)\n",
    "        \n",
    "    X_base = np.array([sample['CRM'][np.triu_indices(sample['CRM'].shape[0])].flatten() for sample in data.samples if data_selector(sample)])\n",
    "    y_base = np.array([get_label_4_tin_thresh2(sample['Group'],sample['Name'],metadata) for sample in data.samples if data_selector(sample)])\n",
    "    svc_base = SVC(kernel='linear')\n",
    "    clf_base = svc_base\n",
    "    \n",
    "    X_lag = np.array([sample['LCM'][np.triu_indices(sample['LCM'].shape[0])].flatten() for sample in data.samples if data_selector(sample)])\n",
    "    y_lag = np.array([get_label_4_tin_thresh2(sample['Group'],sample['Name'],metadata) for sample in data.samples if data_selector(sample)])\n",
    "    svc_lag = SVC(kernel='linear')\n",
    "    clf_lag = svc_lag\n",
    "    \n",
    "    clf_composite = SVC(kernel='linear')\n",
    "    \n",
    "    #Check clustering\n",
    "    #to_TS = make_pipeline(covest,ts)\n",
    "    #X_in_TS = to_TS.transform(X)\n",
    "    #kmeans = KMeans(n_clusters=4,random_state=0).fit(X_in_TS)\n",
    "            \n",
    "    # Monte Carlo, in theory should run this len(y)^2 times, but I need to save my poor computer's memory.\n",
    "    accRiemList = []\n",
    "    accBaseList = []\n",
    "    accLagList = []\n",
    "    accCompList = []\n",
    "    coeffArr = []\n",
    "    matRiemList = []\n",
    "    matBaseList = []\n",
    "    matLagList = []\n",
    "    corrArrBefore = []\n",
    "    corrArrAfter = []\n",
    "\n",
    "    rs = StratifiedShuffleSplit(n_splits=100, test_size=.3)\n",
    "    for i,(train_inds,test_inds) in enumerate(rs.split(X,y)):\n",
    "        X_train, X_test, y_train, y_test = X[train_inds],X[test_inds],y[train_inds],y[test_inds]\n",
    "        X_train_base, X_test_base, y_train_base, y_test_base = X_base[train_inds],X_base[test_inds],y_base[train_inds],y_base[test_inds]\n",
    "        X_train_lag, X_test_lag, y_train_lag, y_test_lag = X_lag[train_inds],X_lag[test_inds],y_lag[train_inds],y_lag[test_inds]\n",
    "        \n",
    "        clf_riem.fit(X_train,y_train)\n",
    "        clf_base.fit(X_train_base,y_train_base)\n",
    "        clf_lag.fit(X_train_lag,y_train_lag)\n",
    "        clf_composite.fit(X_train_lag,y_train_lag)\n",
    "        \n",
    "        #get riemann svm coefficients\n",
    "        coeffArr.append(clf_riem[2].coef_)\n",
    "            \n",
    "        #compare correlation\n",
    "        corr_coeffs_before = np.corrcoef(np.vstack([x[np.triu_indices(33)].flatten() for x in X_train]),rowvar=False)\n",
    "        corrArrBefore.append(np.linalg.norm(corr_coeffs_before))\n",
    "        \n",
    "        ref = ts.reference_\n",
    "        covs = covest.transform(X_train)\n",
    "        mapped = ts.transform(covs)\n",
    "        corr_coeffs_after = np.corrcoef(mapped,rowvar=False)\n",
    "        corrArrAfter.append(np.linalg.norm(corr_coeffs_after))\n",
    "        \n",
    "        \n",
    "        #for flattened in mapped:\n",
    "            \n",
    "            #mat = np.zeros((33,33))\n",
    "            #mat[np.triu_indices(33)] = flattened\n",
    "            #mat = (mat + mat.T)/2\n",
    "            #mapped_mats.append(mat)\n",
    "            \n",
    "        #print(np.mean(np.array([distance_riemann(x,ref) for x in covs])-np.array([np.linalg.norm(x,ord='fro') for x in mapped_mats])/np.array([distance_riemann(x,ref) for x in covs])))\n",
    "        \n",
    "        y_pred = clf_riem.predict(X_test)\n",
    "        y_pred_base = clf_base.predict(X_test_base)\n",
    "        y_pred_lag = clf_lag.predict(X_test_lag)\n",
    "        \n",
    "        # save accuracy\n",
    "        accRiemList.append(accuracy_score(y_pred,y_test))\n",
    "        accBaseList.append(accuracy_score(y_pred_base,y_test_base))\n",
    "        accLagList.append(accuracy_score(y_pred_lag,y_test_lag))\n",
    "        accCompList.append(accuracy_score(clf_composite.predict(X_test_base),y_test_base))\n",
    "        \n",
    "        # confusion matrix\n",
    "        mat = confusion_matrix(y_test,y_pred,normalize='true',labels=[0,1,2,3])\n",
    "        #plot_confusion_matrix(clf_riem, X_test, y_test,normalize='true',labels=['ctr','hl','tin_hl','tin'],display_labels=['ctr','ctr_hl','hl_tin','tin'])\n",
    "        #plt.savefig('RiemannResults/' + fname + '/ConfMatRiemRun' + str(i) + '.png',bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        mat_base = confusion_matrix(y_test_base,y_pred_base,normalize='true',labels=[0,1,2,3])\n",
    "        #plot_confusion_matrix(clf_base, X_test_base, y_test_base,normalize='true',labels=['ctr','hl','tin_hl','tin'],display_labels=['ctr','ctr_hl','hl_tin','tin'])\n",
    "        #plt.savefig('RiemannResults/' + fname + '/ConfMatBaseRun' + str(i) + '.png',bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        mat_lag = confusion_matrix(y_test_lag,y_pred_lag,normalize='true',labels=[0,1,2,3])\n",
    "        #plot_confusion_matrix(clf_lag, X_test_lag, y_test_lag,normalize='true',labels=['ctr','hl','tin_hl','tin'],display_labels=['ctr','ctr_hl','hl_tin','tin'])\n",
    "        #plt.savefig('RiemannResults/' + fname + '/ConfMatLagRun' + str(i) + '.png',bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        \n",
    "        matRiemList.append(mat)\n",
    "        matBaseList.append(mat_base)\n",
    "        matLagList.append(mat_lag)\n",
    "    \n",
    "    for z in range(0,len(coeffArr[0])):\n",
    "        class_z_coeffs = [x[z] for x in coeffArr]\n",
    "        cos_sim = cosine_similarity(class_z_coeffs)\n",
    "        upperTri = cos_sim[np.triu_indices(cos_sim.shape[0],1)]\n",
    "        cos_avg = np.mean(upperTri.flatten())\n",
    "        simArr.append(cos_avg)\n",
    "        \n",
    "    avgMatRiem = sum(matRiemList)/len(matRiemList)\n",
    "    simDict.update({fname: simArr}) \n",
    "    matDict.update({fname: avgMatRiem})\n",
    "    riemAcc = np.mean(accRiemList)\n",
    "    baseAcc = np.mean(accBaseList)\n",
    "    lagAcc = np.mean(accLagList)\n",
    "    compAcc = np.mean(accCompList)\n",
    "    accDict.update({fname:{'riem':riemAcc, 'base': baseAcc,'lag': lagAcc,'comp':compAcc}})\n",
    "    corrDict.update({fname:{'before':np.mean(corrArrBefore),'after':np.mean(corrArrAfter)}})        \n",
    "    print(\"Mean Accuracy w/ Riemann on file \" + fname + \": \" + str(riemAcc))\n",
    "    print(\"Mean Accuracy w/ Base on file \" + fname + \": \" + str(baseAcc))\n",
    "    print(\"Mean Accuracy w/ Lag on file \" + fname + \": \" + str(lagAcc))\n",
    "    print(\"Mean Accuracy w/ Composite on file \" + fname + \": \" + str(compAcc))\n",
    "    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dodtin]",
   "language": "python",
   "name": "conda-env-dodtin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
